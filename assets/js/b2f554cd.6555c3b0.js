"use strict";(self.webpackChunkapache_website_template=self.webpackChunkapache_website_template||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"graphs-openlakehouse","metadata":{"permalink":"/blog/graphs-openlakehouse","editUrl":"https://github.com/apache/incubator-graphar-website/edit/main/blog/2025-09-03-graphs-openlakehouse.mdx","source":"@site/blog/2025-09-03-graphs-openlakehouse.mdx","title":"Dreaming of Graphs in the Open Lakehouse","description":"While Open Lakehouse platforms now natively support tables, geospatial data, vectors, and more, property graphs are still missing. In the age of AI and growing interest in Graph RAG, graphs are becoming especially relevant \u2013 there\u2019s a need to deliver Knowledge Graphs to RAG systems, with standards, ETL, and frameworks for different scenarios. There\u2019s a young project, Apache GraphAr (incubating), that aims to define a storage standard. For processing, there is a good tooling already. GraphFrames is like Spark for Iceberg \u2013 batch and scalable on distributed clusters; Kuzu is like DuckDB for Iceberg \u2013 fast, in-memory, and in-process; Apache HugeGraph is like ClickHouse or Doris for graphs \u2013 a standalone server for queries. All the pieces seem to be here\u2014it just remains to put them together. More thoughts in the full post.","date":"2025-09-03T00:00:00.000Z","formattedDate":"September 3, 2025","tags":[{"label":"graphar","permalink":"/blog/tags/graphar"},{"label":"graphframes","permalink":"/blog/tags/graphframes"},{"label":"kuzu","permalink":"/blog/tags/kuzu"},{"label":"lakehouse","permalink":"/blog/tags/lakehouse"}],"readingTime":13.385,"hasTruncateMarker":true,"authors":[{"name":"Sem Sinchenko","title":"Maintainer of GraphAr","url":"https://semyonsinchenko.github.io/ssinchenko/","imageURL":"https://avatars.githubusercontent.com/u/29755009","key":"ssinchenko"}],"frontMatter":{"slug":"graphs-openlakehouse","title":"Dreaming of Graphs in the Open Lakehouse","authors":["ssinchenko"],"tags":["graphar","graphframes","kuzu","lakehouse"],"image":"/img/blog/graph-lakehouse/graphs-lakehouse.png","description":"While Open Lakehouse platforms now natively support tables, geospatial data, vectors, and more, property graphs are still missing. In the age of AI and growing interest in Graph RAG, graphs are becoming especially relevant \u2013 there\u2019s a need to deliver Knowledge Graphs to RAG systems, with standards, ETL, and frameworks for different scenarios. There\u2019s a young project, Apache GraphAr (incubating), that aims to define a storage standard. For processing, there is a good tooling already. GraphFrames is like Spark for Iceberg \u2013 batch and scalable on distributed clusters; Kuzu is like DuckDB for Iceberg \u2013 fast, in-memory, and in-process; Apache HugeGraph is like ClickHouse or Doris for graphs \u2013 a standalone server for queries. All the pieces seem to be here\u2014it just remains to put them together. More thoughts in the full post."},"unlisted":false,"nextItem":{"title":"Welcome to GraphAr","permalink":"/blog/demo-blog-post"}},"content":"*Initialy published in [Sem Sinchenko\'s blog](https://semyonsinchenko.github.io/ssinchenko/post/dreams-about-graph-in-lakehouse/)*\\n\\n# TLDR;\\n\\nWhile Open Lakehouse platforms now natively support tables, geospatial data, vectors, and more, property graphs are still missing. In the age of AI and growing interest in Graph RAG, graphs are becoming especially relevant \u2013 there\u2019s a need to deliver Knowledge Graphs to RAG systems, with standards, ETL, and frameworks for different scenarios. There\u2019s a young project, Apache GraphAr (incubating), that aims to define a storage standard. For processing, there is a good tooling already. GraphFrames is like Spark for Iceberg \u2013 batch and scalable on distributed clusters; Kuzu is like DuckDB for Iceberg \u2013 fast, in-memory, and in-process; Apache HugeGraph is like ClickHouse or Doris for graphs \u2013 a standalone server for queries. I\u2019m currently working also on graphframes-rs to bring Apache DataFusion and its ecosystem into this picture. All the pieces seem to be here \u2013 it just remains to put them together.\\n\\n\x3c!--truncate--\x3e\\n\\n# Introduction\\n\\n## Open Lakehouse\\n\\nThis isn\u2019t an official definition \u2013 just my personal view of the Open Lakehouse concept. Open Lakehouse is an approach where data lives in shared storage accessible to many systems. The storage can be cloud-based, like Amazon S3 or Azure Blob Storage. It can also be on-premises, such as HDFS, Ceph, or MinIO. There are even solutions aimed at building hybrid cloud + on-prem setups, like IOMETE.\\n\\nDifferent data types use their own open storage standards. For tabular data, there\u2019s Apache Iceberg, Delta Lake, and Apache Hudi. For vectors, Lance can be used. For geospatial data, GeoParquet is one option. Metadata about tables and datasets is kept in a central catalog, like Unity Catalog or Hive Metastore. This allows tools and engines discover where data lives and how it\u2019s structured.\\n\\nDifferent systems are used for different tasks. Apache Spark is great for distributed batch processing of terabytes of data \u2013 it\u2019s not the fastest, but it\u2019s reliable and fault-tolerant. DuckDB is a strong choice for fast, in-memory analytics and is also excellent for single-node ETL tasks. For example, if you have both huge tables (a good case for Spark) and small ones (where spinning up a cluster is overkill), DuckDB is very efficient. Apache Doris and ClickHouse are super solutions for data analysts who need to interactively pull insights from large datasets. For streaming tables and real-time processing, Apache Flink with Apache Paimon is a good fit \u2013 for example, when you need to replicate data from a production PostgreSQL database using Debezium.\\n\\nAll these tools interact with the catalog. They locate, read, process, and write data in standard formats. The catalog also helps with governance and access management, keeping metadata up to date and ensuring everything works together smoothly.\\n\\nThis concept makes it possible to seamlessly connect different tools, always choosing the best one for the job. You don\u2019t have to spin up a cluster just to process a 100 MB table, or rent massive single-node instances (with limited capacity) to crunch terabytes of data by a tool designed for single-node processing. There\u2019s no need to force vector search into pure SQL, either. The Open Lakehouse approach lets you mix and match systems, using each where it fits best, and keeps everything interoperable and efficient.\\n\\n## Property Graphs\\n\\nThis isn\u2019t a textbook definition \u2013 just how I personally see the concept of a property graph. At its core, a graph is a set of **vertices (nodes)** and **edges (links)** connecting them. Edges can be **directed** or **undirected**. They can also be **weighted** or unweighted. Both vertices and edges can have their own **properties** \u2013 key-value pairs with extra information. In practice, we often have **different types of vertices and edges**, each with their own set of properties.\\n\\nThis leads us to the **Property Graph** model \u2013 a graph that contains multiple types of vertices and edges, each of which may have different properties.\\n\\n![Move Fan Social Network](/img/blog/graph-lakehouse/property-graph.png)\\n\\nTo make this concrete, I like the example of a \u201cmovie fan social network.\u201d The diagram above shows how this looks as a property graph. There are **people** and **movies** \u2013 two types of vertices, each with their own properties. People can **like** movies (undirected edges), **send messages** to each other (directed edges), and **follow** directors. There are also **actors** and **directors** as separate vertex types. Movies can be connected as sequels. All these relationships and entities are easy to represent in a property graph, as shown in the diagram.\\n\\nThe property graph model is very universal. Take a **banking payments network**: there are legal entities, government services, individuals, exchanges, and goods. Each is a different vertex type. Payments are directed, weighted edges with properties like date and amount. Two legal entities sharing a board member form an undirected, unweighted edge with the director\u2019s details as properties. This structure is great for compliance, KYC, and anti-fraud. It helps to see who is connected to whom and how closely.\\n\\nOr consider an **online marketplace**. There are buyers, sellers, and products. Buyers purchase products from sellers. Sellers offer many products. All of this fits naturally into a property graph. This structure works well for recommendation systems. Recommending a product is basically a link prediction problem in the graph.\\n\\nAnother example is **Organizational Network Analysis (ONA)**. Companies have departments, teams, and people, all connected in different ways. Teams assign tasks to each other. People have both formal and informal relationships. There are official and real hierarchies. ONA can reveal key employees, process bottlenecks, and even predict conflicts. It also helps improve knowledge sharing across the organization.\\n\\nIn short, the property graph model is flexible and expressive. It works well for many real-world domains.\\n\\n# Property Graphs in the Open Lakehouse\\n\\nTo make property graphs part of the Open Lakehouse, we need three things: a storage standard, a metadata catalog, and tools for different graph tasks. This is just my personal view. With tools, everything is fine\u2014there are plenty of options for ETL-like processing, analytics, ML/AI, and visualization. But with storage standards and catalogs for graphs, things are much less developed. Most of the ecosystem is still focused on tables, and there\u2019s no widely adopted open format or catalog for property graphs yet.\\n\\n## Graph Processing tools\\n\\nLet\u2019s look at three open source tools for working with property graphs. First, GraphFrames. For me, this is like Spark for Apache Iceberg. It scales well and handles huge, long-running batch jobs with reliable distributed processing. Second, Apache HugeGraph (incubating). This is like ClickHouse or Apache Doris for Apache Iceberg. You need a separate server, but you get a great tool for analysts. They can run interactive graph queries using Gremlin or OpenCypher. These are standard query languages for graphs. They let you, for example, find a vertex\u2019s neighborhood up to two hops, or discover all vertices connected through certain types of edges within one to three steps. Third, KuzuDB. I see it as DuckDB for graphs. No separate server is needed. It works in-memory and in-process, with the usual single-node limitations. If your graphs fit into single-node processing, Kuzu is a great tool for both ETL and analytics. It has full OpenCypher support.\\n\\n### GraphFrames\\n\\nSince I\u2019m currently the most active maintainer of GraphFrames, I know this framework best from the inside. I\u2019ve prepared a diagram to show how it works.\\n\\n![GraphFrames top-level overview](/img/blog/graph-lakehouse/gf-overview.png)\\n\\nGraphFrames gives users an API and abstractions for working with graphs, pattern matching, and running algorithms. Under the hood, all these operations are translated into standard relational operations \u2013 select, join, group by, aggregate \u2013 over DataFrames. DataFrames are just data in tabular form. The translated logical plan runs on an Apache Spark cluster. The user always gets results back as a DataFrame, which is simply a table.\\n\\nLet\u2019s look at a concrete example \u2013 PageRank. This algorithm became famous for powering Google Search (fun fact: \u201cPage\u201d is actually the last name of Google co-founder Larry Page, not just about web pages). PageRank helps find the most \u201cimportant\u201d nodes in a graph, like ranking web pages by relevance.\\n\\n![Page Rank in Pregel](/img/blog/graph-lakehouse/pregel-pagerank.png)\\n\\nIn GraphFrames, most algorithms \u2013 including PageRank \u2013 are built on the Pregel framework ([*Malewicz, Grzegorz, et al. \\"Pregel: a system for large-scale graph processing.\\" Proceedings of the 2010 ACM SIGMOD International Conference on Management of data. 2010.*](https://blog.lavaplanets.com/wp-content/uploads/2023/12/p135-malewicz.pdf)). We represent the graph as two DataFrames, which you can think of as tables: one for edges and one for vertices. The PageRank table is initialized by assigning every vertex a starting rank of 0.15.\\n\\nEach iteration of PageRank works like a series of SQL operations. The process starts by joining the edges table with the current PageRank values for each vertex. This creates a triplets table, where each row contains a source, destination, and their current ranks. Next, we generate messages: each source sends its rank to its destination. These messages are grouped by destination and summed up. Finally, we join the results back to the PageRank table and update the rank using a simple formula: `new_rank = sum_rank * 0.85 + 0.15`.\\n\\nThis whole process is repeated \u2013 each step is just a combination of joins, group by, and aggregates over tables \u2013 until the ranks stop changing much. The algorithm converges quickly, usually in about 15\u201320 iterations. Since it relies entirely on SQL operations, running PageRank on an Apache Spark cluster gives you excellent horizontal scalability. As long as your tables fit in Spark, you can compute PageRank using Pregel. In practice, this means you can almost infinitely scale just by adding more hardware.\\n\\n### Kuzu DB\\n\\nI should mention up front that I know KuzuDB only as a user, so everything here is based on their documentation and public materials.\\n\\nKuzuDB is an embedded, in-process graph database designed for speed and analytics on a single machine. It stores graph data using a columnar format, including a columnar adjacency list\u2014basically, a fast and compressed way to represent which nodes are connected. This approach enables extremely fast joins and efficient analytical queries, even for complex graph workloads. All data is stored on disk in a columnar layout, which helps with both speed and compression. As a result, KuzuDB delivers high performance for graph analytics without needing a separate server. If your graph fits on a single node, KuzuDB is a great choice for ETL, analytics, and any workload where you want fast, in-memory processing with full OpenCypher support.\\n\\n### Apache HugeGraph (incubating)\\n\\nI should state upfront that I\'m only superficially familiar with Apache HugeGraph, and most of what I know about its architecture comes from the documentation. Apache HugeGraph is not like Kuzu or GraphFrames, as it requires a standalone server to run on.\\n\\n![Apache HugeGraph Architecture](https://hugegraph.apache.org/docs/images/design/architectural-revised.png)\\n\\nHugeGraph has three main layers. The **Application Layer** includes tools like Hubble for visualization, Loader for importing data, command-line tools, and Computer \u2013 a distributed OLAP engine based on Pregel (yes, that\u2019s the same Pregel framework I described in the GraphFrames section). There\u2019s also a client library for developers.\\n\\nThe **Graph Engine Layer** is the core server. It exposes a REST API and supports both Gremlin and OpenCypher queries. This layer handles both transactional (OLTP) and analytical (OLAP) workloads.\\n\\nThe **Storage Layer** is flexible. You can choose different backends, like RocksDB, MySQL, or HBase, depending on your needs. This modular design lets you scale from embedded setups to the distributed storage. Overall, HugeGraph is built for both interactive and analytical graph workloads, with full support for Gremlin and OpenCypher.\\n\\n## Storage: Apache GraphAr (incubating)\\n\\nThe only standard for Property Graph storage that I know of today is **Apache GraphAr (incubating)** (GraphAr means Graph Archive). I should add that I\u2019m a member of the GraphAr PPMC committee, but I\u2019ve honestly searched for other attempts to create a similar standard and haven\u2019t found any.\\n\\nThe core idea behind GraphAr is simple: you can think of it as Delta Lake, but for graphs. Data is stored in a columnar format\u2014using Apache Parquet or Apache ORC files. Alongside the data, there are human-readable YAML metadata files. GraphAr represents Property Graphs as logical **Property Groups** for both vertices and edges. Each group has its own schema (properties), keys for vertices, and attributes like edge directionality. Internally, there are unique LONG indices for vertices and edges. The format is optimized for query engines: for example, there are edge offset tables for every vertex.\\n\\n![Apache GraphAr Property Graph model example](/img/docs/property_graph.png)\\n\\nThe optimization principle is based on the fact that real-world graphs are usually clustered. By sorting data by vertex, we achieve excellent compression in Parquet, since properties and their values tend to be similar within clusters. Metadata, grouping, and data sorting open up a lot of room for query optimization. The optimizer can push down entire groups and skip reading files using min-max metadata in Parquet headers.\\n\\n![Apache GraphAr Vertex Storage Model](/img/docs/vertex_physical_table.png)\\n\\nRight now, there\u2019s a reference C++ API for GraphAr. There\u2019s also an Apache Spark and PySpark API, which I\u2019m actively helping to develop. A standalone Java API (not tied to Apache Spark) is in progress. There\u2019s even a CLI tool, modeled after Parquet Inspector, built on top of the C++ API.\\n\\n![Apache GraphAr Edges Storage Model](/img/docs/edge_logical_table.png)\\n\\n** Brining all together\\n\\nBringing everything together, the architecture looks surprisingly clean and modular.\\n\\n![Possible integration of Property Graphs to the Open Lakehouse](/img/blog/graph-lakehouse/graphs-lakehouse.png)\\n\\nAt the center is the storage standard - GraphAr - which acts as the \u201cDelta Lake for graphs.\u201d All graph data, whether from batch analytics or interactive workloads, is stored in a unified, open format. Around this core, different tools plug in depending on your needs. GraphFrames provides scalable, distributed analytics and ETL on Spark. KuzuDB offers fast, in-process analytics for single-node workloads. HugeGraph covers the interactive, OLAP, and OLTP scenarios with full support for Gremlin and OpenCypher. Each tool can read and write to GraphAr, so you\u2019re never locked into one engine or workflow.\\n\\nThis modular approach means you can mix and match tools as your requirements change. For example, you might use Spark and GraphFrames for heavy ETL and analytics, then switch to KuzuDB for fast, local exploration, or to HugeGraph for interactive graph queries and visualization. The open storage format ensures that your data remains portable and future-proof, no matter which engine you choose.\\n\\nNow for the current status. After several years in maintenance mode, I and other enthusiasts have revived GraphFrames and started releasing new versions. The brand new []Property Graph model](https://github.com/graphframes/graphframes/blob/master/core/src/main/scala/org/graphframes/propertygraph/PropertyGraphFrame.scala) that is fully aligned with GraphAr is already merged and will be available on the next release. After that it will be easy to add a support of reading and writing to GraphAr from GraphFrames.\\n\\nFor KuzuDB, the prototype is already [exists](https://github.com/kuzudb/kuzu/issues/5903)\\n\\nAs for HugeGraph, the team is waiting for GraphAr\u2019s Java API to stabilize. Once that happens, they\u2019re interested in integrating with the format as well.\\n\\n# Bonus Part 1: catalog integration\\n\\nIn theory, you could also add a catalog layer to this picture. Options like Apache Polaris or other Iceberg Catalogs probably won\u2019t work \u2013 they\u2019re focused only on tables and Apache Iceberg. But solutions like Unity Catalog seem much more promising and could likely be extended to support graphs. I even [asked about this in the Unity repo](https://github.com/unitycatalog/unitycatalog/discussions/252), though I haven\u2019t received a response yet. Since Unity is written in Java, I could potentially add this integration myself if the maintainers are open to the idea.\\n\\n# Bonus Part 2: Apache DataFusion integration\\n\\nRight now, I\u2019m also working on bringing graphs into the Apache DataFusion ecosystem by rewriting GraphFrames in Rust. I\u2019ve just started, but the [Core Pregel](https://github.com/SemyonSinchenko/graphframes-rs/blob/main/src/pregel.rs) engine is already done \u2013 and it even passes tests. The next step is to implement the graph algorithms themselves.\\n\\n# Conclusion\\n\\nIn the end, I dream of seeing graphs become a natural part of the Open Lakehouse ecosystem. I believe this would unlock a whole new set of possibilities \u2013 especially now, in the age of AI, Graph-RAG, and Knowledge Graphs. And it\u2019s not just a dream \u2013 I\u2019m actively working to make it real.\\nI hope it was interesting to read and I will be happy to hear any feedback!"},{"id":"demo-blog-post","metadata":{"permalink":"/blog/demo-blog-post","editUrl":"https://github.com/apache/incubator-graphar-website/edit/main/blog/2024-04-08-demo-blog-post.mdx","source":"@site/blog/2024-04-08-demo-blog-post.mdx","title":"Welcome to GraphAr","description":"Welcome to the GraphAr Blog! The blog about GraphAr will come soon. Stay tuned!","date":"2024-04-08T00:00:00.000Z","formattedDate":"April 8, 2024","tags":[{"label":"graphar","permalink":"/blog/tags/graphar"}],"readingTime":0.07,"hasTruncateMarker":false,"authors":[{"name":"Weibin Zeng","title":"Maintainer of GraphAr","url":"https://github.com/acezen","imageURL":"https://github.com/acezen.png","key":"acezen"}],"frontMatter":{"slug":"demo-blog-post","title":"Welcome to GraphAr","authors":["acezen"],"tags":["graphar"]},"unlisted":false,"prevItem":{"title":"Dreaming of Graphs in the Open Lakehouse","permalink":"/blog/graphs-openlakehouse"}},"content":"Welcome to the GraphAr Blog! The blog about GraphAr will come soon. Stay tuned!"}]}')}}]);